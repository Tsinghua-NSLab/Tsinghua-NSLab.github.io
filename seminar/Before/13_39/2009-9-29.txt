Topic: R-ADMAD: High Reliability Provision for Large-Scale De-duplication Archival Storage Systems
Speaker: Chuanyi Liu
Attendants: Prof. Li, Prof. Xue, Zhen Chen, Jin Zhou, Ying Zhang, Baohua Yang, Hui Zhang, Li Tang, Jeffrey Fong
Recorder: Li Tang 
Content:
<1> Background
Digital Explosion
281 exabytes(2007) --> 1773 exabytes(2011)
Data Duplication
75% of the digital universe is a copy/replicate/duplicate/backup/mirror image/redundant (IDC Survey 2008)
Data De-duplication
What: Refer to a kind of approach that uses Lossless Data Compression Algorithms to minimize the duplicate data at the inter-file level
Why: 
Data duplication & redundancy have serious impacts
extra storage spaces
more power consumption
lower down storage utilization
extra burden on management
Help reduce the amount of data sent over networks<2> Challenges
De-duplication and reliability provision are intrinsically contradicting to one another
Data chunks are usually variable sized after the chunking procedure; while most of the reliability schemes work on fixed-size blocks
As each data chunk may be shared by many files, load balancing is also an important factor that should be considered<3>Key Ideas of R-ADMAD
Divide the storage nodes into dynamic redundancy groups
Pack the de-duped variable-length data chunks into fixed sized objects
Exploit ECC codes to encode the objects and distributes them among the storage nodes in a redundancy group
Propose a distributed and dynamic recovery process<4>Evaluation
Reliability
Storage space utilization
Recovery performance <5>Conclusion
R-ADMAD is the first attempt to design a high-reliability mechanism for de-duplication storage systems by exploiting ECC
R-ADMAD proposes a mechanism to pack variable sized data chunks into fixed sized objects, which are convenient for redundancy schemes to work on
R-ADMAD uses the commodity components to build Intelligent Storage Nodes (ISN), which are used as distributed storage servers
Experimental results show that R-ADMAD can provide
the same storage utilization as RAID-like schemes, but comparable reliability to replication based schemes with much more redundancies. 
the average recovery time about 2-5 times less than RAID-like schemes. 
run-time dynamic load balance even without the involvement of the overloaded storage nodes.
Discussion:
1. 李老师： Chunk的size为什么要变化，如果size是变化的，Object的大小至少要取最大的Chunk的size？
川意： 变长是为了提高Chunk在切分时的重复概率；Object的大小需要根据经验配置和选择，一个Object可能包含多个Chunk。
2. 李老师： 这种切法是不是跟Data Leakage Prevention的Finger Print方法有联系？
川意： 工业界提出一种基于内容存储的概念。
3. 保华： 是递增性的添加文件吗？在起初未知数据时怎样选择参数？
川意： 有个学习的过程，管理员需要预先进行实验，根据数据特点合理配置参数。
4. 保华： 为什么选择IP网络？
川意： 成本低，跟光纤等其它技术比有价格优势。
5. 保华： 数据中心内部的网络跟Internet不同，是否其它协议更有效？
川意： Berkeley正在进行此方面研究，并提出了新的协议。
6. 保华： 怎样划分Chunk，怎样部署Object？对哪个环节进行了改进？
川意： 本文为关注切分问题，以前的工作曾提出基于语义进行切分的想法。本文主要讨论如何对Object进行编码
7. 周晋： 如何向用户证明云的安全性？用户不相信服务商进行加密。
川意： 目前云存储的主要应用是归档。
李老师： 服务商加密是为了在硬盘发生故障而丢弃或销毁时，不会泄露用户的数据。
8. 薛老师： 在二进制层面上，不同加密方法是否会产生不同特征的文件？
川意： 不同加密算法的性能不同，但二进制特征未发现明显不同。
9. 李老师： 如果存储介质是磁带，无论是读写特性，还是功耗都会发生根本变化，因此，这些技术的前提假设也就发生了变化。
川意： 的确，这些技术是针对磁盘介质设计的。
10. 周晋： 关于De-dup的研究是否有必要考虑文件类型。
川意： 我以前有一篇文章就是研究这个内容，工业界对这项技术的需求很迫切。
